# my global config
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          # - alertmanager:9093
          # - "my-alertmanager:9093" # 因為 Prometheus 與 alertmanager 是用同一個 docker-compose 啟動的(同一個 docker-network)，故可以直接用 container-name 訪問

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
# - "first_rules.yml"
# - "second_rules.yml"
# - "./rules/roger_rules_group1.yml"
# - "./rules/roger_rules_group2.yml"

## A scrape configuration containing exactly one endpoint to scrape:
## Here it's Prometheus itself.
#scrape_configs:
#  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
#  - job_name: "prometheus"
#
#    # metrics_path defaults to '/metrics'
#    # scheme defaults to 'http'.
#
#    static_configs:
#      - targets: ["localhost:9090"]

############################################################
# static_configs 靜態配置
# 每次配置都需要重啟 Prometheus 服務
# 使用 http://172:20:10:2:9090/targets 可查看 Targets
############################################################
#scrape_configs:
#  - job_name: 'prometheus-job'
#    static_configs: # 靜態配置方式，每次配置都需要重啟 Prometheus 服務
#      - targets: ['172.20.10.2:9090'] # 查詢本機IP: ifconfig | grep "inet " | grep -v 127.0.0.1
#        labels:
#          instance: prometheus
#          service: prometheus-service
#
#  - job_name: 'node-exporter-job'
#    static_configs: # 靜態配置方式，每次配置都需要重啟 Prometheus 服務
#      - targets: ['172.20.10.2:9100'] # 查詢本機IP: ifconfig | grep "inet " | grep -v 127.0.0.1
#        labels:
#          instance: node-exporter
#          service: node-exporter-service

############################################################
# File-based service discovery
# 只要文件發生變化，Prometheus 就會自動加載
# 使用 http://172:20:10:2:9090/targets 可查看 Targets
############################################################
scrape_configs:
  - job_name: 'prometheus-job'
    file_sd_configs:
      # - files: ['/usr/local/prometheus/groups/prometheus-job/*.json'] # 使用 json
      - files: ['/usr/local/prometheus/groups/prometheus-job/*.yaml'] # 使用 yaml

#  - job_name: 'redis_exporter_targets'
#    scrape_interval: 15s
#    scrape_timeout: 10s
#    static_configs: # 靜態配置方式，每次配置都需要重啟 Prometheus 服務
#      - targets:
#        - "redis://my-redis-gpa:6379"
#    # metrics_path: /scrape
#    metrics_path: /metrics # 指定抓取指標的路徑
#    relabel_configs:
#      # 將 __address__ 的值複製到 __param_target 標籤中
#      - source_labels: [__address__]
#        target_label: __param_target
#      # 將 __param_target 的值複製到 instance 標籤中
#      - source_labels: [__param_target]
#        target_label: instance
#      # 將 __address__ 的值替換為指定的新地址和端口
#      - target_label: __address__
#        replacement: my-redis-exporter:9121

  # config for scraping the exporter itself
  - job_name: 'redis_exporter'
    scrape_interval: 15s
    scrape_timeout: 10s
#    static_configs: # 靜態配置方式，每次配置都需要重啟 Prometheus 服務
#      - targets:
#        - my-redis-exporter:9121 # 此處 ip 要寫桌機的 IP，不能寫 localhost，因為是使用 docker 運行 Prometheus，localhost 會被視為 Prometheus container 自身
    file_sd_configs: # 動態配置方式(需在 prometheus 容器掛 volume)
      - files:
          - '/usr/local/prometheus/groups/redis-exporter-job/*.yaml'